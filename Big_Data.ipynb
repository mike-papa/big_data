{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7eYRF9b+eixcylPA5zdqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mike-papa/big_data/blob/main/Big_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install -q pyspark==3.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pWsdAry8XPCn",
        "outputId": "6edf3e97-bce5-4bfb-9855-5285f4b247df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 4 <br>\n",
        "Załaduj plik pracownicy.csv z poprawnie zdefiniowaną strukturą i typami danych (schema)\n",
        "Dla każdego roku urodzenia występującego w danych wylicz średnią stawkę godzinową\n",
        "posortuj od najmłodszych lat do najstarszych\n",
        "Wynik zapisz do excela"
      ],
      "metadata": {
        "id": "6jrW88LtWLPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jusz6_fOVwHE",
        "outputId": "0dfe234d-8ea1-44c1-9879-5281d03d0f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+---------------------------+-------------+\n",
            "|     imie| nazwisko|stawka_godzinowa_w_dolarach|rok_urodzenia|\n",
            "+---------+---------+---------------------------+-------------+\n",
            "|   Maciej| Kowalski|                       78.0|         1944|\n",
            "|  Michael| Kowalski|                       57.0|         1965|\n",
            "|      Ewa| Kowalska|                       23.0|         1999|\n",
            "|     John|    Nowak|                       12.0|         2020|\n",
            "|    Steve| Stevniak|                       67.0|         1955|\n",
            "|     Bill|    Gates|                       67.0|         1955|\n",
            "|     Jan | Kowalski|                       23.0|         1999|\n",
            "|     John|    Nowak|                       12.0|         2020|\n",
            "|    Steve| Stevniak|                       67.0|         1955|\n",
            "|     Bill|    Gates|                       67.0|         1955|\n",
            "|      Jan| Kowalski|                       23.0|         1999|\n",
            "|     John|    Nowak|                       12.0|         2020|\n",
            "|    Steve|Stevniak |                       67.0|         1955|\n",
            "|Magdalena|    Gates|                       67.0|         1955|\n",
            "|     John|    Nowak|                       78.0|         1944|\n",
            "|    Steve| Stevniak|                       57.0|         1965|\n",
            "|     Bill|    Gates|                       23.0|         1999|\n",
            "|      Jan| Kowalski|                       12.0|         2020|\n",
            "|Michalina|    Nowak|                       67.0|         1955|\n",
            "|    Steve| Stevniak|                       67.0|         1955|\n",
            "+---------+---------+---------------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------+------------------------+\n",
            "|rok_urodzenia|średnia_stawka_godzinowa|\n",
            "+-------------+------------------------+\n",
            "|         1944|                    78.0|\n",
            "|         1955|                    67.0|\n",
            "|         1965|                    57.0|\n",
            "|         1999|                    23.0|\n",
            "|         2020|                    12.6|\n",
            "+-------------+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "\n",
        "# Inicjalizacja Spark Session\n",
        "spark = SparkSession.builder.appName(\"Przetwarzanie danych pracowników\").getOrCreate()\n",
        "\n",
        "# Ścieżka do pliku CSV\n",
        "file_path = 'pracownicy.csv'\n",
        "\n",
        "# Definicja schematu\n",
        "schema = StructType([\n",
        "    StructField(\"imie\", StringType(), True),\n",
        "    StructField(\"nazwisko\", StringType(), True),\n",
        "    StructField(\"stawka_godzinowa_w_dolarach\", FloatType(), True),\n",
        "    StructField(\"rok_urodzenia\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Załadowanie danych\n",
        "df = spark.read.csv(file_path, schema=schema, header=True, sep=';')\n",
        "\n",
        "df.show(20)\n",
        "\n",
        "# Obliczanie średniej stawki godzinowej dla każdego roku urodzenia\n",
        "avg_stawka_df = df.groupBy(\"rok_urodzenia\").agg(avg(\"stawka_godzinowa_w_dolarach\").alias(\"średnia_stawka_godzinowa\")).orderBy(\"rok_urodzenia\")\n",
        "avg_stawka_df.show()\n",
        "# Konwersja do Pandas DataFrame\n",
        "result_df = avg_stawka_df.toPandas()\n",
        "\n",
        "# Zapis do pliku Excel\n",
        "result_df.to_excel(\"srednie_stawki_godzinowe.xlsx\", index=False)\n",
        "\n",
        "# Zamknięcie Spark Session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 5 <br>\n",
        "Jakiego dnia ile było wejść a angielską wikipedię? Rozwiąż na dwa sposoby (wykorzystując czyste RDD oraz Dataframes). 4 kolumna w danych oznacza ile bylo wejsc. <br>\n",
        "Plik : pagecounts"
      ],
      "metadata": {
        "id": "ZsE3hG4HaTLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "files = \"pagecounts/part-*\"  # Ścieżka do plików danych\n",
        "rdd = sc.textFile(files)\n",
        "\n",
        "def process_line(line):\n",
        "    parts = line.split()\n",
        "    date = parts[0][:8] # wyciąganie daty z timestamp\n",
        "    lang = parts[1] # język\n",
        "    page_views = int(parts[3]) # liczba wyświetleń strony\n",
        "    return (date, lang, page_views)\n",
        "\n",
        "# Przetwarzanie linii\n",
        "rdd_processed = rdd.map(process_line)\n",
        "\n",
        "for record in rdd_processed.take(5):\n",
        "    print(record)\n",
        "\n",
        "# Filtrowanie tylko wpisów dla 'en'\n",
        "rdd_en = rdd_processed.filter(lambda x: x[1] == 'en')\n",
        "\n",
        "for record in rdd_en.take(5):\n",
        "    print(record)\n",
        "\n",
        "# Agregacja liczby wejść dla każdego dnia\n",
        "rdd_aggregated = rdd_en.map(lambda x: (x[0], x[2])).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "print(rdd_aggregated.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb8LpfUWYAw3",
        "outputId": "1aee3ffc-05a5-4415-f5ba-f2c993d293ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('20090505', 'aa', 2)\n",
            "('20090505', 'ab', 1)\n",
            "('20090505', 'ab', 1)\n",
            "('20090505', 'af.b', 1)\n",
            "('20090505', 'af.d', 4)\n",
            "('20090505', 'en', 4)\n",
            "('20090505', 'en', 21)\n",
            "('20090505', 'en', 9)\n",
            "('20090505', 'en', 2)\n",
            "('20090505', 'en', 4)\n",
            "[('20090505', 7076855), ('20090507', 6175726)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Inicjalizacja SparkSession\n",
        "spark = SparkSession.builder.appName(\"Wikipedia Page Views\").getOrCreate()\n",
        "\n",
        "# Wczytanie danych do DataFrame\n",
        "df = spark.read.csv(\"pagecounts/part-*\", sep=\" \").toDF(\"timestamp\", \"lang\", \"title\", \"page_views\", \"size\")\n",
        "\n",
        "df.show(5)\n",
        "\n",
        "# Filtrowanie danych dla języka angielskiego\n",
        "df_en = df.filter(col(\"lang\") == \"en\")\n",
        "\n",
        "df_en.show(5)\n",
        "\n",
        "# Grupowanie danych po dacie i sumowanie liczby wejść\n",
        "df_aggregated = df_en.groupBy(df_en.timestamp.substr(1, 8).alias(\"date\")).agg(sum(\"page_views\").alias(\"total_views\"))\n",
        "\n",
        "# Wyświetlenie wyników\n",
        "df_aggregated.show()\n",
        "\n",
        "# Zamykanie Spark Session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4DjkOVNjd3F",
        "outputId": "8b79e0d9-84ad-4844-93b9-6c9da22e5f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----+--------------------+----------+------+\n",
            "|      timestamp|lang|               title|page_views|  size|\n",
            "+---------------+----+--------------------+----------+------+\n",
            "|20090505-000000|  aa|           Main_Page|         2|  9980|\n",
            "|20090505-000000|  ab|%D0%90%D0%B8%D0%B...|         1|   465|\n",
            "|20090505-000000|  ab|%D0%98%D1%85%D0%B...|         1| 16086|\n",
            "|20090505-000000|af.b|            Tuisblad|         1| 36236|\n",
            "|20090505-000000|af.d|            Tuisblad|         4|189738|\n",
            "+---------------+----+--------------------+----------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+----+--------------------+----------+------+\n",
            "|      timestamp|lang|               title|page_views|  size|\n",
            "+---------------+----+--------------------+----------+------+\n",
            "|20090505-000000|  en|                   !|         4|170494|\n",
            "|20090505-000000|  en|                 !!!|        21|306957|\n",
            "|20090505-000000|  en|      !!!Fuck_You!!!|         9| 87025|\n",
            "|20090505-000000|  en|!!!Fuck_You!!!_an...|         2| 17960|\n",
            "|20090505-000000|  en|         !!!_(album)|         4| 16665|\n",
            "+---------------+----+--------------------+----------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------+-----------+\n",
            "|    date|total_views|\n",
            "+--------+-----------+\n",
            "|20090505|  7076855.0|\n",
            "|20090507|  6175726.0|\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 6 <br>\n",
        "Wczytaj wiki_sample.parquet (moodle)\n",
        "Odpowiedz na pytanie którzy użytkownicy edytowali najwięcej stron na wikipedii (top 5)?\n",
        "Wykonaj zadanie na dwa sposoby (Dataframe SQL oraz Dataframe programmatic API)"
      ],
      "metadata": {
        "id": "bYjb4XuWs1LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#brak pliku wiki_sample.parquet na moodle"
      ],
      "metadata": {
        "id": "FCx-tZ0zuxiw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}